{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/stanford-natural-language-inference-corpus/README.txt\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_train.csv\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_dev.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/stanford-natural-language-inference-corpus/snli_1.0_train.csv\", nrows=100000)\nvalid_df = pd.read_csv(\"../input/stanford-natural-language-inference-corpus/snli_1.0_dev.csv\")\ntest_df = pd.read_csv(\"../input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")","execution_count":5,"outputs":[{"output_type":"stream","text":"Total train samples : 100000\nTotal validation samples: 10000\nTotal test samples: 10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[['sentence1','sentence2','label1']]\nvalid_df = valid_df[['sentence1','sentence2','label1']]\n\ntest_df = test_df[['sentence1','sentence2','label1']]\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\nprint(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\nprint(f\"Similarity: {train_df.loc[1, 'label1']}\")","execution_count":7,"outputs":[{"output_type":"stream","text":"Sentence1: A person on a horse jumps over a broken down airplane.\nSentence2: A person is at a diner, ordering an omelette.\nSimilarity: contradiction\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace=True)","execution_count":8,"outputs":[{"output_type":"stream","text":"Number of missing values\nsentence1    0\nsentence2    3\nlabel1       0\ndtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Target Distribution\")\nprint(train_df.label1.value_counts())","execution_count":9,"outputs":[{"output_type":"stream","text":"Train Target Distribution\nentailment       33333\ncontradiction    33332\nneutral          33332\nName: label1, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation Target Distribution\")\nprint(valid_df.label1.value_counts())","execution_count":10,"outputs":[{"output_type":"stream","text":"Validation Target Distribution\nentailment       3334\ncontradiction    3333\nneutral          3333\nName: label1, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = (\n    train_df[train_df.label1 != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)\nvalid_df = (\n    valid_df[valid_df.label1 != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"label\"] = train_df[\"label1\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"label1\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"label1\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n\n    sequence_output, pooled_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True)\n    )(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\n\nprint(f\"Strategy: {strategy}\")\nmodel.summary()","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e647dc4e403481ebe7956f9cf74d747"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0482470e6d5145b58e9b6c83d70e20fe"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f5af72ad090>\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n                                                                 attention_masks[0][0]            \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 109,909,507\nTrainable params: 427,267\nNon-trainable params: 109,482,240\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c90906688ad4f838c34db878a5da058"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)","execution_count":16,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/2\n3124/3124 [==============================] - ETA: 0s - loss: 0.6952 - acc: 0.7037","name":"stdout"},{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"},{"output_type":"stream","text":"3124/3124 [==============================] - 668s 214ms/step - loss: 0.6952 - acc: 0.7037 - val_loss: 0.5568 - val_acc: 0.7766\nEpoch 2/2\n3124/3124 [==============================] - 661s 212ms/step - loss: 0.5924 - acc: 0.7602 - val_loss: 0.5225 - val_acc: 0.7960\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = BertSemanticDataGenerator(\n    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_test,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.evaluate(test_data, verbose=1)","execution_count":26,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"},{"output_type":"stream","text":"312/312 [==============================] - 55s 177ms/step - loss: 0.5249 - acc: 0.7891\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"[0.5248708724975586, 0.7890625]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_similarity(sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data)[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx]: .2f}%\"\n    pred = labels[idx]\n    return pred, proba","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"Two women are observing something together.\"\nsentence2 = \"Two women are standing with their eyes closed.\"\ncheck_similarity(sentence1, sentence2)","execution_count":28,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"('entailment', ' 0.61%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"A smiling costumed woman is holding an umbrella\"\nsentence2 = \"A happy woman in a fairy costume holds an umbrella\"\ncheck_similarity(sentence1, sentence2)","execution_count":29,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"('neutral', ' 0.72%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"A soccer game with multiple males playing\"\nsentence2 = \"Some men are playing a sport\"\ncheck_similarity(sentence1, sentence2)","execution_count":30,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"('entailment', ' 0.93%')"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}